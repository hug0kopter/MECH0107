{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15cacdc5-6a27-4aea-97bb-6ba2fec12db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729a669d-10af-4edf-acdc-72334dc3ce62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= Uncomment for Full Reproducibility (good for debugging or model comparisons) =======\n",
    "# seed = 42\n",
    "# random.seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# tf.random.set_seed(seed)\n",
    "# tf.config.experimental.enable_op_determinism()  # Enable deterministic operations in TensorFlow\n",
    "# tf.keras.backend.clear_session()  # Set TensorFlow backend to deterministic mode\n",
    "\n",
    "# ======= Load Dataset =======\n",
    "filename = \"Dataset_BeamNaturalFrequency.txt\"\n",
    "df = pd.read_csv(filename, sep='\\t')\n",
    "X = df[['E (Pa)', 'I (m^4)', 'ρ (kg/m^3)', 'A (m^2)', 'L (m)']].values  # Inputs\n",
    "Y = df[['f (Hz)']].values  # Output\n",
    "\n",
    "# ======= Data Preprocessing =======\n",
    "# Split into training, validation, and test sets (70% train, 15% val, 15% test)\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Standardize inputs and outputs\n",
    "scaler_X = StandardScaler()\n",
    "scaler_Y = StandardScaler()\n",
    "\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_val = scaler_X.transform(X_val)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "\n",
    "Y_train = scaler_Y.fit_transform(Y_train)\n",
    "Y_val = scaler_Y.transform(Y_val)\n",
    "Y_test = scaler_Y.transform(Y_test)\n",
    "\n",
    "# ======= Build Neural Network =======\n",
    "model = Sequential([\n",
    "    layers.Input(shape=(X_train.shape[1],)),  # Input Layer\n",
    "    layers.Dense(10, activation='relu'),  # Hidden Layer - try 'sigmoid', 'tanh', 'relu', or 'leaky_relu'\n",
    "    layers.Dense(1)  # Output Layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.01), loss='MSE', metrics=['MAPE'])  # Try different optimisers (SGD, Adam etc.)\n",
    "\n",
    "# ======= Train Model with Early Stopping =======\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    patience=10,  # Stop after 10 epochs with no improvement\n",
    "    restore_best_weights=True  # Restore the best weights after stopping\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    validation_data=(X_val, Y_val),\n",
    "    epochs=1000,\n",
    "    batch_size=32,  # Try different batch sizes (16, 32, 64, 128, etc.)\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# ======= Model Evaluation =======\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Convert predictions back to original scale\n",
    "def inverse_transform_and_evaluate(X, Y, scaler_Y, dataset_name):\n",
    "    Y_pred = model.predict(X)\n",
    "    Y_pred_original = scaler_Y.inverse_transform(Y_pred)\n",
    "    Y_original = scaler_Y.inverse_transform(Y)\n",
    "\n",
    "    # Mean Absolute Percentage Error (MAPE)\n",
    "    MAPE = np.mean(np.abs((Y_original - Y_pred_original) / Y_original)) * 100\n",
    "    print(f'\\n MAPE_{dataset_name} (%)', MAPE)\n",
    "\n",
    "    return Y_original, Y_pred_original, MAPE\n",
    "\n",
    "# Evaluate on training, validation, and test sets\n",
    "Y_train_original, Y_train_pred_original, MAPE_train = inverse_transform_and_evaluate(X_train, Y_train, scaler_Y, \"train\")\n",
    "Y_val_original, Y_val_pred_original, MAPE_val = inverse_transform_and_evaluate(X_val, Y_val, scaler_Y, \"val\")\n",
    "Y_test_original, Y_test_pred_original, MAPE_test = inverse_transform_and_evaluate(X_test, Y_test, scaler_Y, \"test\")\n",
    "\n",
    "\n",
    "# # ======= Visualization =======\n",
    "# # Loss vs. Epochs\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(train_loss, label='Training Loss', color='blue')\n",
    "# plt.plot(val_loss, label='Validation Loss', color='orange')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss (MSE)')\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.yscale('log')  # Log scale for better visualization\n",
    "# plt.show()\n",
    "\n",
    "# # Scatter plot of Actual vs. Predicted for Validation Set\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.scatter(Y_val_original, Y_val_pred_original, alpha=0.5, label='Predicted vs Actual')\n",
    "# plt.plot([Y_val_original.min(), Y_val_original.max()],\n",
    "#          [Y_val_original.min(), Y_val_original.max()],\n",
    "#          color='red', linestyle='--', label='Ideal Fit')  # Line of perfect predictions\n",
    "# plt.xlabel('Actual δ (m)')\n",
    "# plt.ylabel('Predicted δ (m)')\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
